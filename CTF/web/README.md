# web

check:

- robots.txt
- check default credentials for proprietary software
- source code (look for comments, links)
- headers (do they change when sending with different methods?)
- images of webpages, open them with editor
- cookies and local storage
- security tab, certificates
- whenever you see a long weird string (even in paths) -> try to crack it
- iterate id's in url
